{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we capture the variability in both precip. and prob. of precip. with a single NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from torch.distributions import Beta\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.set_default_device(device) ## set global device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import matplotlib\n",
    "import datetime as dt\n",
    "import glob\n",
    "import bisect\n",
    "import zipfile\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## no. of cpu workers\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Data_loader\n",
    "importlib.reload(Data_loader)\n",
    "from Data_loader import PrcNorm, ThermoNorm, LoadTraining, Normalize, CustomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 0) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_PATH='/ocean/projects/ees220002p/fiaz/'\n",
    "IMERG_ERA5_PATH=PROJ_PATH+'ocn/'\n",
    "PRC_PATH=IMERG_ERA5_PATH+'prc_ocn/prc_oceans_2015_01_01.npy'\n",
    "HBL_PATH=IMERG_ERA5_PATH+'hbl_ocn/hbl_oceans_2015_01_01.npy'\n",
    "HLFT_PATH=IMERG_ERA5_PATH+'hlft_ocn/hlft_oceans_2015_01_01.npy'\n",
    "HSAT_LFT_PATH=IMERG_ERA5_PATH+'hsat_lft_ocn/hsat_lft_oceans_2015_01_01.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imerg_prc=PrcNorm(PRC_PATH)\n",
    "imerg_prc.compute_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_thermo=ThermoNorm(HBL_PATH,HLFT_PATH,HSAT_LFT_PATH)\n",
    "era5_thermo.compute_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Declare directories ###\n",
    "fils_hbl=glob.glob(IMERG_ERA5_PATH+'hbl_ocn/hbl_oceans_*npy')\n",
    "fils_hbl.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_hlft_dir=IMERG_ERA5_PATH+'hlft_ocn/'\n",
    "era5_hsat_lft_dir=IMERG_ERA5_PATH+'hsat_lft_ocn/'\n",
    "imerg_prc_dir=IMERG_ERA5_PATH+'prc_ocn/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prc_norm_dict={'prc_mean':0,\n",
    "               'prc_std':imerg_prc.precipitating_std}\n",
    "\n",
    "thermo_norm_dict={'instab_mean':era5_thermo.instab_mean,\n",
    "                 'instab_std':era5_thermo.instab_std,\n",
    "                 'subsat_mean':era5_thermo.subsat_mean,\n",
    "                 'subsat_std':era5_thermo.subsat_std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_locations={'hlft_dir':era5_hlft_dir,\n",
    "              'hlft_sat_dir':era5_hsat_lft_dir,\n",
    "              'prc_dir':imerg_prc_dir,\n",
    "               'prc_norm':prc_norm_dict, \n",
    "               'thermo_norm':thermo_norm_dict,\n",
    "              'batch_size':BATCH_SIZE}\n",
    "\n",
    "Dataset=torch.utils.data.Dataset\n",
    "Dataloader=torch.utils.data.DataLoader\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    def __init__(self,fils:str,\n",
    "                 hlft_dir:str,hlft_sat_dir:str,prc_dir:str,\n",
    "                 prc_norm:dict,thermo_norm:dict,\n",
    "                batch_size:int):\n",
    "        \n",
    "        self.fils=fils\n",
    "        self.hlft_dir=hlft_dir\n",
    "        self.hlft_sat_dir=hlft_sat_dir\n",
    "        self.prc_dir=prc_dir\n",
    "        \n",
    "        ## get norm. values from dict ###\n",
    "        self.prc_mean = prc_norm['prc_mean']\n",
    "        self.prc_std = prc_norm['prc_std']\n",
    "        \n",
    "        self.instab_mean = thermo_norm['instab_mean']\n",
    "        self.instab_std = thermo_norm['instab_std']\n",
    "        \n",
    "        self.subsat_mean = thermo_norm['subsat_mean']\n",
    "        self.subsat_std = thermo_norm['subsat_std']\n",
    "        \n",
    "        #############\n",
    "        \n",
    "        self.total_size=0 ## store total number of samples\n",
    "        self.fil_bounds=[] ## store bounds to identify each npy file\n",
    "        \n",
    "        for fil in self.fils:\n",
    "            fil_len=(np.load(fil,mmap_mode='r').size//batch_size)*batch_size\n",
    "            self.total_size+=fil_len \n",
    "            self.fil_bounds.append(self.total_size) \n",
    "            \n",
    "    def __len__(self)->int:\n",
    "        return self.total_size\n",
    "    \n",
    "    def __normalize(self, x,xbar,normalizer):\n",
    "        return (x-xbar)/normalizer\n",
    "\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        ## get fil_idx and array_idx using idx ##\n",
    "        fil_idx=[idx//i for i in self.fil_bounds].index(0) ## get file to open\n",
    "        fil=self.fils[fil_idx]\n",
    "        \n",
    "        if fil_idx>0:\n",
    "            array_idx=idx-self.fil_bounds[fil_idx-1] \n",
    "        else:\n",
    "            array_idx=idx\n",
    "        \n",
    "        ##open files ###\n",
    "        date_str=fil.split('hbl_oceans_')[-1].split('.npy')[0]\n",
    "        \n",
    "        fils_prc=self.prc_dir+f\"prc_oceans_{date_str}.npy\"\n",
    "        fils_hlft=self.hlft_dir+f\"hlft_oceans_{date_str}.npy\"\n",
    "        fils_hsat_lft=self.hlft_sat_dir+f\"hsat_lft_oceans_{date_str}.npy\"\n",
    "\n",
    "        hbl=np.load(fil,mmap_mode='r')[array_idx]\n",
    "        hlft=np.load(fils_hlft,mmap_mode='r')[array_idx]\n",
    "        hsat_lft=np.load(fils_hsat_lft,mmap_mode='r')[array_idx]\n",
    "        prc=np.load(fils_prc,mmap_mode='r')[array_idx]\n",
    "        \n",
    "                \n",
    "        ## compute instab and subsat\n",
    "        instab=(hbl-hsat_lft)*340./hsat_lft\n",
    "        subsat=(hsat_lft-hlft)*340./hsat_lft\n",
    "        \n",
    "        ### normalize data ###\n",
    "        \n",
    "        instab=self.__normalize(instab, self.instab_mean,self.instab_std)\n",
    "        subsat=self.__normalize(subsat, self.subsat_mean,self.subsat_std)\n",
    "        prc=self.__normalize(prc, self.prc_mean,self.prc_std)\n",
    "        \n",
    "        return instab, subsat, prc\n",
    "        \n",
    "training_data=DataGenerator(fils_hbl[:2],**fil_locations)\n",
    "training_generator=torch.utils.data.DataLoader(training_data, batch_size=BATCH_SIZE, \n",
    "                           shuffle=True, num_workers=0,\n",
    "                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 14327808 samples in 55968 batches\n"
     ]
    }
   ],
   "source": [
    "print(f'Training {len(training_data)} samples in {len(training_generator)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataloader output ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "\n",
    "instab=[]\n",
    "subsat=[]\n",
    "prc=[]\n",
    "\n",
    "for i_batch, samples in enumerate(training_generator):\n",
    "    \n",
    "    instab.append(samples[0].detach().numpy())\n",
    "    subsat.append(samples[1].detach().numpy())\n",
    "    prc.append(samples[2].detach().numpy())\n",
    "    \n",
    "    if i_batch==50:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(instab)[:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axx=plt.subplots(1,3,figsize=(8,2.5))\n",
    "\n",
    "ax=axx[0]\n",
    "ax.hist(np.concatenate(instab).flatten())\n",
    "\n",
    "ax=axx[1]\n",
    "ax.hist(np.concatenate(subsat).flatten())\n",
    "\n",
    "ax=axx[2]\n",
    "# ax.hist(np.concatenate(conv_prc)*prc_norm_dict['normalizer'].flatten(),density=True)\n",
    "ax.hist(np.concatenate(prc),density=True)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(right=20)\n",
    "# ax=axx[1,0]\n",
    "# ax.hist(np.concatenate(conv_nn_prc)*prc_norm.flatten())\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "# ax=axx[1,1]\n",
    "# ax.hist(np.concatenate(imerg_prc_tm1)*prc_norm.flatten())\n",
    "# ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Vae_model' from '/jet/home/fiaz/precip-vae/Vae_model.py'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Vae_model \n",
    "import sys\n",
    "importlib.reload(Vae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIMS=1\n",
    "INPUT_DIMS=3\n",
    "NN_DIMS_ENC=12\n",
    "NN_DIMS_DEC=12\n",
    "\n",
    "model=Vae_model.CVAE_ORG_mod(LATENT_DIMS,INPUT_DIMS,\n",
    "                                             NN_DIMS_ENC,NN_DIMS_DEC).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=1e-4,\n",
    "                            weight_decay=1e-3)\n",
    "# \n",
    "# train_opt=torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of parameters: 365\n"
     ]
    }
   ],
   "source": [
    "from operator import mul\n",
    "from functools import reduce # python3 compatibility\n",
    "\n",
    "network_size=0\n",
    "for params in model.parameters():\n",
    "    network_size+=reduce(mul, params.size(), 1)\n",
    "print('total number of parameters: {:d}'.format(network_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "startTime=datetime.now()\n",
    "\n",
    "for i_batch, sample_batched in enumerate(training_generator):\n",
    "    \n",
    "    \n",
    "    data=torch.stack(sample_batched,dim=1).unsqueeze(2)\n",
    "    \n",
    "    ## transfer to GPU\n",
    "\n",
    "    print([f\"{round(i*1e-9,3)} GB\" for i in torch.cuda.mem_get_info()])\n",
    "    sTime=datetime.now()\n",
    "    data=data.float().to(device)\n",
    "    print(f\"GPU transfer took {(datetime.now() - sTime).total_seconds()/60} minutes\")\n",
    "    print([f\"{round(i*1e-9,3)} GB\" for i in torch.cuda.mem_get_info()])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs=model(data)\n",
    "    kl_loss, binary_loss, gamma_loss, gaussian_loss\\\n",
    "    =model.vae_loss(outputs)\n",
    "#     , gamma_nll_1, gamma_nll_2, gamma_nll_3, gamma_nll_4\\  \n",
    "    \n",
    "\n",
    "    syn_size=1_000_000\n",
    "    z=torch.normal(mean=0.,std=1.,\n",
    "                                size=(syn_size,LATENT_DIMS)).to(device)\n",
    "    \n",
    "    input_tensor=torch.cat((torch.tensor([1.0]).unsqueeze(1),\n",
    "                        torch.tensor([-1]).unsqueeze(1)),axis=1).to(device)    \n",
    "    \n",
    "    Vae_model.print_params(z,model,input_tensor)\n",
    "\n",
    "    break\n",
    "    \n",
    "print(f\"Took {(datetime.now() - startTime).total_seconds()/60} minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_W=0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"precip-VAE\",\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "#     \"architecture\": \"VAE\",\n",
    "#     \"dataset\": \"GPM/ERA5\",\n",
    "#     \"epochs\": EPOCHS,\n",
    "#     \"batch size\":BATCH_SIZE,\n",
    "#     \"NN DIMD\": NN_DIMS_DEC,\n",
    "#     \"NN_DIME\": NN_DIMS_ENC,\n",
    "#     \"Latent DIM\": LATENT_DIMS,\n",
    "#     \"Predictor\": \"lrh\",\n",
    "#     \"alpha_w\":ALPHA_W,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 0\n",
      "Total time: 0.05 minutes\n",
      "EPOCH 1:\n",
      "0\n",
      "Took: 7.000565528869629e-05 seconds\n",
      "1\n",
      "Took: 7.77076929807663e-05 seconds\n",
      "2\n",
      "Took: 7.763132452964783e-05 seconds\n",
      "3\n",
      "Took: 0.00010573118925094604 seconds\n",
      "4\n",
      "Took: 7.231347262859344e-05 seconds\n",
      "5\n",
      "Took: 7.672049105167389e-05 seconds\n",
      "6\n",
      "Took: 7.48559832572937e-05 seconds\n",
      "7\n",
      "Took: 7.524341344833374e-05 seconds\n",
      "8\n",
      "Took: 7.181428372859955e-05 seconds\n",
      "9\n",
      "Took: 7.04638659954071e-05 seconds\n",
      "10\n",
      "Took: 0.00024301744997501373 seconds\n",
      "11\n",
      "Took: 0.0002592019736766815 seconds\n",
      "12\n",
      "Took: 0.00024289824068546295 seconds\n",
      "13\n",
      "Took: 7.419288158416748e-05 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 31\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001B[39;00m\n\u001B[1;32m     29\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     30\u001B[0m     mean_ELBO, mean_gamma_loss, mean_gaussian_loss, mean_KL_loss, mean_binary_loss\\\n\u001B[0;32m---> 31\u001B[0m     \u001B[38;5;241m=\u001B[39m \u001B[43mVae_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_number\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtraining_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m                                                \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mALPHA_W\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m#     losses['elbo'].append(mean_ELBO)\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m#     losses['gamma'].append(mean_gamma_loss)\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m#     losses['gauss'].append(mean_gaussian_loss)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \n\u001B[1;32m     40\u001B[0m     \u001B[38;5;66;03m# We don't need gradients on to do reporting\u001B[39;00m\n\u001B[1;32m     41\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/precip-vae/Vae_model.py:209\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(epoch_index, training_generator, model, optimizer, ALPHA_W)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;66;03m# z=torch.normal(mean=0.,std=1.,size=(1000_00,1)).to(device)\u001B[39;00m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i_batch, sample_batched \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(training_generator):\n\u001B[0;32m--> 209\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize()\n\u001B[1;32m    210\u001B[0m     sTime0\u001B[38;5;241m=\u001B[39mtime\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m    212\u001B[0m     data\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mstack(sample_batched,dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/mambaforge/envs/ml_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    636\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/mambaforge/envs/ml_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    676\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 677\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    678\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    679\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/mambaforge/envs/ml_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/mambaforge/envs/ml_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[15], line 67\u001B[0m, in \u001B[0;36mDataGenerator.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     64\u001B[0m fils_hlft\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhlft_dir\u001B[38;5;241m+\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhlft_oceans_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdate_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     65\u001B[0m fils_hsat_lft\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhlft_sat_dir\u001B[38;5;241m+\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhsat_lft_oceans_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdate_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 67\u001B[0m hbl\u001B[38;5;241m=\u001B[39m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfil\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmmap_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m[array_idx]\n\u001B[1;32m     68\u001B[0m hlft\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mload(fils_hlft,mmap_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m)[array_idx]\n\u001B[1;32m     69\u001B[0m hsat_lft\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mload(fils_hsat_lft,mmap_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m)[array_idx]\n",
      "File \u001B[0;32m~/mambaforge/envs/ml_310/lib/python3.10/site-packages/numpy/lib/npyio.py:453\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[1;32m    451\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m allow_pickle:\n\u001B[1;32m    452\u001B[0m         max_header_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m64\u001B[39m\n\u001B[0;32m--> 453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen_memmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmmap_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    454\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mmax_header_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_header_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m.\u001B[39mread_array(fid, allow_pickle\u001B[38;5;241m=\u001B[39mallow_pickle,\n\u001B[1;32m    457\u001B[0m                              pickle_kwargs\u001B[38;5;241m=\u001B[39mpickle_kwargs,\n\u001B[1;32m    458\u001B[0m                              max_header_size\u001B[38;5;241m=\u001B[39mmax_header_size)\n",
      "File \u001B[0;32m~/mambaforge/envs/ml_310/lib/python3.10/site-packages/numpy/lib/format.py:925\u001B[0m, in \u001B[0;36mopen_memmap\u001B[0;34m(filename, mode, dtype, shape, fortran_order, version, max_header_size)\u001B[0m\n\u001B[1;32m    922\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    923\u001B[0m     \u001B[38;5;66;03m# Read the header of the file first.\u001B[39;00m\n\u001B[1;32m    924\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os_fspath(filename), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[0;32m--> 925\u001B[0m         version \u001B[38;5;241m=\u001B[39m \u001B[43mread_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    926\u001B[0m         _check_version(version)\n\u001B[1;32m    928\u001B[0m         shape, fortran_order, dtype \u001B[38;5;241m=\u001B[39m _read_array_header(\n\u001B[1;32m    929\u001B[0m                 fp, version, max_header_size\u001B[38;5;241m=\u001B[39mmax_header_size)\n",
      "File \u001B[0;32m~/mambaforge/envs/ml_310/lib/python3.10/site-packages/numpy/lib/format.py:235\u001B[0m, in \u001B[0;36mread_magic\u001B[0;34m(fp)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_magic\u001B[39m(fp):\n\u001B[1;32m    224\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Read the magic string to get the version of the file format.\u001B[39;00m\n\u001B[1;32m    225\u001B[0m \n\u001B[1;32m    226\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03m    minor : int\u001B[39;00m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 235\u001B[0m     magic_str \u001B[38;5;241m=\u001B[39m \u001B[43m_read_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMAGIC_LEN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmagic string\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m magic_str[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m!=\u001B[39m MAGIC_PREFIX:\n\u001B[1;32m    237\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe magic string is not correct; expected \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/mambaforge/envs/ml_310/lib/python3.10/site-packages/numpy/lib/format.py:966\u001B[0m, in \u001B[0;36m_read_bytes\u001B[0;34m(fp, size, error_template)\u001B[0m\n\u001B[1;32m    961\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    962\u001B[0m     \u001B[38;5;66;03m# io files (default in python3) return None or raise on\u001B[39;00m\n\u001B[1;32m    963\u001B[0m     \u001B[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001B[39;00m\n\u001B[1;32m    964\u001B[0m     \u001B[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001B[39;00m\n\u001B[1;32m    965\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 966\u001B[0m         r \u001B[38;5;241m=\u001B[39m \u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    967\u001B[0m         data \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m r\n\u001B[1;32m    968\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(r) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m==\u001B[39m size:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "startTime1 = datetime.now()\n",
    "SAVE_MODEL=False\n",
    "MODEL_NAME_STR='cvae_tr2_imerg_era5_gamma_gauss_alpha_w={}'.format(ALPHA_W)\n",
    "\n",
    "fig,axx=plt.subplots(1,1,figsize=(6,4))\n",
    "ax=axx\n",
    "losses={'elbo':[],'gamma':[], \n",
    "        'gauss':[],'binary':[], 'kl':[]}\n",
    "\n",
    "set_seed()\n",
    "epoch_number = 0\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "syn_size=1_000_000\n",
    "\n",
    "z=torch.normal(mean=0.,std=1.,size=(syn_size,LATENT_DIMS)).to(device)\n",
    "\n",
    "input_tensor=torch.cat((torch.tensor([1.0]).unsqueeze(1),\n",
    "                    torch.tensor([-1]).unsqueeze(1)),axis=1).to(device)    \n",
    "\n",
    "print(\"Total time: {:.2f} minutes\".format((datetime.now() - startTime1).total_seconds()))\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    startTime2 = datetime.now()\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    mean_ELBO, mean_gamma_loss, mean_gaussian_loss, mean_KL_loss, mean_binary_loss\\\n",
    "    = Vae_model.train_one_epoch(epoch_number,training_generator,model,\n",
    "                                                optimizer,ALPHA_W)\n",
    "    \n",
    "#     losses['elbo'].append(mean_ELBO)\n",
    "#     losses['gamma'].append(mean_gamma_loss)\n",
    "#     losses['gauss'].append(mean_gaussian_loss)\n",
    "#     losses['binary'].append(mean_binary_loss)\n",
    "#     losses['kl'].append(mean_KL_loss)\n",
    "    \n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "    \n",
    "    Vae_model.print_params(z,model,input_thermo_tensor)\n",
    "\n",
    "#     ax.scatter(epoch,mean_ELBO,color='black')\n",
    "#     ax.scatter(epoch,mean_gamma_loss,color='red')\n",
    "#     ax.scatter(epoch,mean_KL_loss,color='blue')\n",
    "#     ax.scatter(epoch,mean_binary_loss,color='orange')\n",
    "    \n",
    "    epoch_number += 1\n",
    "    print(\"Time for epoch: {:.2f} minutes\".format((datetime.now() - startTime2).total_seconds()/60))\n",
    "        \n",
    "#     wandb.log({\"ELBO\": mean_ELBO, \"gamma loss\": mean_gamma_loss,\n",
    "#                \"gaussian loss\":mean_gaussian_loss,\n",
    "#               \"binary loss\": mean_binary_loss, \"KL loss\": mean_KL_loss})\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "        dir_name='/ocean/projects/ees220002p/fiaz/trained_models/'\n",
    "\n",
    "        model_name_prev=MODEL_NAME_STR+'_{}_epochs.pth'.format(epoch_number-1)\n",
    "        model_name=MODEL_NAME_STR+'_{}_epochs.pth'.format(epoch_number)\n",
    "\n",
    "        if epoch_number>1:\n",
    "            os.remove(dir_name+model_name_prev)\n",
    "        torch.save(model.state_dict(), dir_name+model_name)\n",
    "        print('Model saved as {}'.format(dir_name+model_name))\n",
    "        \n",
    "print(\"Total time: {:.2f} minutes\".format((datetime.now() - startTime1).total_seconds()/60))\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small scale parameters suggest small tails. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst='/home/fiaz/ML/vae/models/cvae_gamma_conv_rain_singleED_50_epochs.pth'\n",
    "# dst='/home/fiaz/ML/vae/models/cvae_gamma_conv_rain_singleED_NN=4_LD=1_48_epochs.pth'\n",
    "# dst='/home/fiaz/ML/vae/models/cvae_gamma_conv_rain_singleED_NN=4_LD=2_7_epochs.pth'\n",
    "# model.load_state_dict(torch.load(dst))\n",
    "Vae_model.print_params(z,model,torch.tensor([0.8]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcp_bins=2**(np.arange(-2.,8.125,0.125))\n",
    "pcp_bins=np.insert(pcp_bins,0,1e-3)\n",
    "pcp_bin_center=(pcp_bins[1:]+pcp_bins[:-1])*0.5\n",
    "dx=np.diff(pcp_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "colors_norm1 = matplotlib.colors.Normalize(vmin=0, vmax=1.)\n",
    "cmap = plt.get_cmap('YlOrRd')\n",
    "col = matplotlib.cm.ScalarMappable(norm=colors_norm1, cmap=cmap)\n",
    "\n",
    "colors_norm = matplotlib.colors.Normalize(vmin=-3, vmax=3.)\n",
    "colz = matplotlib.cm.ScalarMappable(norm=colors_norm, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_size=1_000_00\n",
    "synthetic_input=torch.normal(mean=0.,std=1.,\n",
    "                            size=(syn_size,LATENT_DIMS))\n",
    "\n",
    "\n",
    "fig,axx=plt.subplots(2,2,figsize=(8.,5.))\n",
    "\n",
    "for i in np.arange(0,0.95,.05)[::4]: \n",
    "    print(i)\n",
    "#     synthetic_input[:,0]=3\n",
    "    synthetic_input[:]=4.\n",
    "\n",
    "    crh_cond=torch.ones([syn_size,1])\n",
    "    crh_cond[:]=i\n",
    "    \n",
    "    log_alpha,log_beta, prob=model.decoder(synthetic_input,crh_cond)\n",
    "    m1=torch.distributions.Gamma(log_alpha.exp(),log_beta.exp())\n",
    "    m2=torch.distributions.Bernoulli(prob)\n",
    "    \n",
    "    prc_array= (m2.sample()*m1.sample()*prc_std).numpy().squeeze()\n",
    "    \n",
    "    prc_hist=np.histogram(prc_array,bins=pcp_bins)[0]\n",
    "    prc_hist=prc_hist/(dx*prc_hist.sum())\n",
    "    \n",
    "    axx[0,0].scatter(pcp_bin_center,prc_hist,color=col.to_rgba(i))\n",
    "    axx[0,1].hist(prob.detach().numpy().squeeze(),color=col.to_rgba(i))\n",
    "    \n",
    "#     if i==.5:\n",
    "#         break\n",
    "    \n",
    "    \n",
    "for i in np.arange(-3,3.2,0.2):\n",
    "    \n",
    "#     synthetic_input[:,0]=i\n",
    "    synthetic_input[:]=i\n",
    "\n",
    "    crh_cond=torch.ones([syn_size,1])\n",
    "    crh_cond[:]=0.8\n",
    "    \n",
    "    log_alpha, log_beta, prob=model.decoder(synthetic_input,crh_cond)\n",
    "    m1=torch.distributions.Gamma(log_alpha.exp(),log_beta.exp())\n",
    "    m2=torch.distributions.Bernoulli(prob)\n",
    "    \n",
    "    prc_array= (m2.sample()*m1.sample()*prc_norm).numpy().squeeze()\n",
    "    \n",
    "    prc_hist=np.histogram(prc_array,bins=pcp_bins)[0]\n",
    "    prc_hist=prc_hist/(dx*prc_hist.sum())\n",
    "    \n",
    "    axx[1,0].scatter(pcp_bin_center,prc_hist,color=colz.to_rgba(i))\n",
    "    axx[1,1].hist(prob.detach().numpy().squeeze(),color=colz.to_rgba(i))\n",
    "    \n",
    "    \n",
    "axx[0,0].set_yscale('log')\n",
    "axx[1,0].set_yscale('log')\n",
    "axx[0,0].set_xlim(0,250)\n",
    "axx[1,0].set_xlim(0,250)\n",
    "\n",
    "for ax in axx[0]:\n",
    "    ax.tick_params(which='both',labelsize=13)\n",
    "\n",
    "for ax in axx[1]:\n",
    "    ax.tick_params(which='both',labelsize=13)\n",
    "\n",
    "# axx[0,1].set_xlabel('Shape',fontsize=13)\n",
    "# axx[1,1].set_xlabel('Scale',fontsize=13)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_310",
   "language": "python",
   "name": "ml_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
